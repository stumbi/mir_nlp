{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8c109bd",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 align=\"center\">Excercise 03 - Medical Information Retrieval 2023</h1>\n",
    "  </div>\n",
    "  <br />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f85263",
   "metadata": {},
   "source": [
    "Before you start with this exercise:\n",
    "\n",
    "In the last weeks exercise, we went through the NLP-steps data-exploration, data cleaning and tokenization. In the end of this exercise we want to test a simple machine learning model, which takes your preprocessed texts as input. Therefore, we suggest that, before you start, you copy your code from data cleaning inside this notebook and continue with the following tasks. **A tokenizer is not needed in this exercise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fba12f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty    object\n",
       "transcription        object\n",
       "cleaned              object\n",
       "tokenized            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv(\"../DATA/mtsamples_clean.csv\")\n",
    "\n",
    "df['cleaned'] = df['transcription'].apply(lambda row: re.sub(r\"[\\W\\d\\s]\", ' ', str(row)).lower())\n",
    "df['tokenized'] = df['cleaned'].apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1089a22",
   "metadata": {},
   "source": [
    "## NLP Pipeline - Part 2 <a class=\"anchor\" id=\"first\"></a>\n",
    "\n",
    "This week, we further focus on two common preprocessing steps in the NLP pipeline: stemming/lemmatization and stop word analysis.\n",
    "\n",
    "### Stemming/Lemmatization\n",
    "Stemming and lemmatization are techniques used to reduce inflectional and derivational forms of words to their base or root form, in order to simplify the analysis of text data. In this task, we will use the nltk library to perform stemming and lemmatization on the preprocessed text data.\n",
    "\n",
    "Please perform the following steps:\n",
    "\n",
    "* Choose a stemmer from the nltk library and apply it to the tokenized text in the dataset.\n",
    "* Choose a lemmatizer from the nltk library and apply it to the tokenized text in the dataset.\n",
    "* Compare the results of the stemming and lemmatization techniques on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a02378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code ###\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "df['stemmed'] = df['tokenized'].apply(lambda x: [stemmer.stem(i) for i in list(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9715178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(i) for i in list(x)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0689ee33",
   "metadata": {},
   "source": [
    "### Stop Word Analysis\n",
    "\n",
    "Stop words are words that are commonly used in a language, but do not add any value to the meaning of a sentence. Examples of stop words in English include \"the\", \"and\", \"a\", \"an\", \"in\", etc. In this task, we will remove stop words from the preprocessed text data in order to improve the quality of our classification results.\n",
    "\n",
    "Please perform the following steps:\n",
    "\n",
    "* Use the nltk library to obtain a list of stop words in English.\n",
    "* Remove the stop words from the preprocessed text data in the dataset.\n",
    "* Compare the results of the classification with and without stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36e88f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code ###\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords\n",
    "\n",
    "df['stopwords_removed'] = df['lemmatized'].apply(lambda x: [word for word in x if word not in eng_stopwords])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb38495c",
   "metadata": {},
   "source": [
    "### Test your preprocessing steps!\n",
    "\n",
    "It is crucial to test the effectiveness of our preprocessing techniques on the accuracy of a machine learning classifier, as the quality of the data fed into the model can have a significant impact on the model's performance. If the input data is noisy, contains irrelevant information, or is not properly transformed, the accuracy of the resulting model will suffer. Therefore, it is important to **evaluate the impact of our preprocessing techniques on the classification task.**\n",
    "\n",
    "In this exercise, we provide you with a simple baseline model for text classification using scikit-learn. Now, it's time to test the effectiveness of your techniques you learned so-far on the classification accuracy.\n",
    "\n",
    "To do this, we recommend that you first select a subset of the dataset and preprocess it using the techniques learned in the previous sections. Then, train a machine learning classifier on the preprocessed data using the given pipeline. Finally, evaluate the accuracy of the classifier on the preprocessed data and compare it with the accuracy of the same classifier trained on the raw data.\n",
    "\n",
    "The following train-function takes the pandas dataframe as input and trains a naive-bayes classifier on it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35aa0e00",
   "metadata": {},
   "source": [
    "Note, that the following train-function takes a pandas dataframe as input. **You should keep the column-names for notes (\"transcription\") and classes (\"medical_specialty\").**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e7b6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "### do not change! ###\n",
    "def train(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['stopwords_removed'], df['medical_specialty'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the text classification pipeline\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(lowercase=False)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    # Train the pipeline on the preprocessed data\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = text_clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    accuracy = (predicted == y_test).mean()\n",
    "    f1 = f1_score(y_test, predicted, average='micro')\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "##############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d504c557",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mstopwords_removed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mstopwords_removed\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mlist\u001b[39m(x)))\n\u001b[0;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mmedical_specialty\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mmedical_specialty\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m train(df)\n",
      "Cell \u001b[1;32mIn[82], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     13\u001b[0m text_clf \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     14\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, CountVectorizer(lowercase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)),\n\u001b[0;32m     15\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, MultinomialNB())\n\u001b[0;32m     16\u001b[0m ])\n\u001b[0;32m     18\u001b[0m \u001b[39m# Train the pipeline on the preprocessed data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m text_clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     21\u001b[0m predicted \u001b[39m=\u001b[39m text_clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Evaluate the accuracy of the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    360\u001b[0m     cloned_transformer,\n\u001b[0;32m    361\u001b[0m     X,\n\u001b[0;32m    362\u001b[0m     y,\n\u001b[0;32m    363\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    364\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    365\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    366\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "df['stopwords_removed'] = df['stopwords_removed'].apply(lambda x: ' '.join(list(x)))\n",
    "df['medical_specialty'] = df['medical_specialty'].astype(str)\n",
    "\n",
    "train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "47bcf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['stopwords_removed'], df['medical_specialty'], test_size=0.2, random_state=42)\n",
    "X_train_org, X_test_org, y_train_org, y_test_org = train_test_split(df['transcription'], df['medical_specialty'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a7c48bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_removed: subjective year old white female present complaint allergy used allergy lived seattle think worse past ha tried claritin zyrtec worked short time seemed lose effectiveness ha used allegra also used last summer began using two week ago doe appear working well ha used counter spray prescription nasal spray doe asthma doest require daily medication doe think flaring medication medication currently ortho tri cyclen allegra allergy ha known medicine allergy objective vitals weight wa pound blood pressure heent throat wa mildly erythematous without exudate nasal mucosa wa erythematous swollen clear drainage wa seen tm clear neck supple without adenopathy lung clear assessment allergic rhinitis plan try zyrtec instead allegra another option use loratadine doe think ha prescription coverage might cheaper sample nasonex two spray nostril given three week prescription wa written well \n",
      "originial: SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n"
     ]
    }
   ],
   "source": [
    "print('stopwords_removed:', X_train[0], '\\noriginial:', X_train_org[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02fd3572",
   "metadata": {},
   "source": [
    "In order to train and test the machine learnign model on your preprocessed dataset, the dataframe should have the same column names as in the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
