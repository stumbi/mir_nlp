{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1089a22",
   "metadata": {},
   "source": [
    "  <div>\n",
    "    <h1 align=\"center\">Excercise 05 - Medical Information Retrieval 2023</h1>\n",
    "  </div>\n",
    "  <br />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa3db9f9",
   "metadata": {},
   "source": [
    "Today, we are moving on, towards a machine learning approach for text classification. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e603d07c",
   "metadata": {},
   "source": [
    "## Text classification <a class=\"anchor\" id=\"first\"></a>\n",
    "\n",
    "In the following 3 weeks we are focussing on machine learning approaches on our classification task. Feel free to use any tool which helps you, as long as you can explain, what exactelly is happening, and why it is useful. Given, that you know the preprocessing steps from the past weeks and are able to apply them, we want you to use them now in order to develop a machine learning model for our classification problem.\n",
    "\n",
    "### Requirements\n",
    "* The notebook should run **without any error**, given that all packages are installed and the dataset is loaded. When we test it, we will adapt path definitions and might will install nessesary packages)\n",
    "* Your training/validation script should only use the train split we give you.\n",
    "\n",
    "### Evaluation\n",
    "* For evalutation, you can use the function \"test_model_performance\" in this notebook for accuracy, precision, recall and F1-score. If you choose to use such evaluation, the predicted labels have to be hot-encoded: The output of your model should be a vector of probabilities for each class. \n",
    "### Your tasks\n",
    "\n",
    "* Make an exploratory data analysis\n",
    "* Develop a preprocessing pipeline\n",
    "* Train and test one or several machine learning models\n",
    "* Evaluate the algorithms with a metric of your choice \n",
    "* Visualize the outcome\n",
    "\n",
    "* Prepare a presentation (or present this notebook) of around 10 minutes for our last session (6th of June)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95e2c944",
   "metadata": {},
   "source": [
    "You can start from here. To have a comparable evaluation between each group, we give you a fixed train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee480cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading the dataset ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/02-NLP-Pipeline/mtsamples_clean.csv')\n",
    "\n",
    "### creating train and test split ###\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_X = df['transcription']\n",
    "_y = df['medical_specialty']\n",
    "_y_one_hot = pd.get_dummies(_y)\n",
    "\n",
    "X, X_test, y_one_hot, _ = train_test_split(_X, _y_one_hot, test_size=0.2, random_state=123)\n",
    "_, _, y_classes, _ = train_test_split(_X, _y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2444db14",
   "metadata": {},
   "source": [
    "Only use X and y_one_hot or y_classes for training purposes in the rest of the notebook. After running the whole notebook, there should be a prediction from your model, which took X_test as input to create the predictions. Each prediction has to be a vector of length 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2e7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def test_model_performance(y_pred):\n",
    "    _, _, _, y_test = train_test_split(_X, _y_one_hot, test_size=0.2, random_state=123)\n",
    "\n",
    "    # set highest to 1 and rest to 0\n",
    "    #y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1: ', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a16090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.03\n",
      "Precision:  0.08513340376412133\n",
      "Recall:  0.03\n",
      "F1:  0.036686564580444364\n"
     ]
    }
   ],
   "source": [
    "### performance of a random guesser ###\n",
    "\n",
    "y_pred_dummy = np.zeros((len(X_test), 40))\n",
    "# random predictions with sum 1\n",
    "y_pred_dummy = y_pred_dummy + np.random.rand(y_pred_dummy.shape[0], y_pred_dummy.shape[1])\n",
    "y_pred_dummy = y_pred_dummy / y_pred_dummy.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "# set only highest value to 1 and rest to 0\n",
    "y_pred_dummy = np.argmax(y_pred_dummy, axis=1)\n",
    "y_pred_dummy = pd.get_dummies(y_pred_dummy).values\n",
    "\n",
    "test_model_performance(y_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4e90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
